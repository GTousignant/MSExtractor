<a name="readme-top"></a>

<h3 align="center">MSExtractor</h3>

  <p align="center">
    This project is an implementation of the MSExtractor decomposition approach as described in the paper "Improving microservices 
    extraction using evolutionary search" <a href="https://doi.org/10.1016/j.infsof.2022.106996">(2022)</a>.

  </p>



<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li>
      <a href="#usage">Usage</a>
      <ul>
        <li><a href="#server">Starting the server</a></li>
        <li><a href="#cli">Using the MSExtractor CLI</a></li>
        <li><a href="#cli">Using a third-party analysis tool</a></li>
      </ul>
    </li>
    <li><a href="#license">License</a></li>
    <li><a href="#references">References</a></li>
    <li><a href="#citation">Citation</a></li>
  </ol>
</details>



<!-- ABOUT THE PROJECT -->
# About The Project

This project is an implementation of the MSExtractor decomposition approach as described in the paper 
"Improving microservices extraction using evolutionary search" [[1]](#1).

MSExtractor is a decomposition tool that analyzes the source code of a monolithic Java application and suggests the 
recommended microservices for each class in the system using the evolutionary algorithm IBEA 
(Indicator-Based Evolutionary Algorithm). 

The MSExtractor Python module contains the logic for generating the decomposition of the monolithic application. 
However, the analysis of the source code has to be done by another tool. This implementation is compatible with 
the packages [decomp-java-analysis-service](https://github.com/khaledsellami/decomp-java-analysis-service) and [decomp-parsing-service](https://github.com/khaledsellami/decomp-parsing-service.git) that handle the static analysis part of the process.
Otherwise, it is possible to use your own tool but the input to MSExtractor has to conform to the required types and structure.



# Getting Started

There are multiple ways to use MSExtractor: with the Docker images or as a Python module.

## Prerequisites

This is an example of how to list things you need to use the software and how to install them.
* Python 3.10 or higher
* Docker (Optional for running the parser or analysis service in Docker containers)

## Installation (Python module)

1. Clone the repo
   ```sh
   git clone https://github.com/khaledsellami/MSExtractor.git
   ```
2. Install the Python libraries
   ```sh
   cd MSExtractor/
   pip install .
   ```



<!-- USAGE EXAMPLES -->
# Usage

## With Docker
The current MSExtractor image can be launched as a server or can be used to generate a single decomposition through 
the command line interface.


We can run MSExtractor as a gRPC server. In this case, the decomposition can be generated by using a gRPC client using 
the protobuf file "msextractor.proto". MSExtractor will be able to download and decompose the source code 
of the monolithic directly from its Github repository.

To start the server you simply need to use the docker-compose.yml file:
   ```sh
   docker compose up
   ```

The server will be accessible from http://localhost:50060


## With the Command Line Interface (CLI)
In this case, the analysis data has to be generated using the analysis and parsing tools images locally and then given 
to MSExtractor as input.

### With the analysis and parsing tools:

1. Start by running the analysis service and the parsing service. The instructions can be found in the repositories [decomp-java-analysis-service](https://github.com/khaledsellami/decomp-java-analysis-service) and [decomp-parsing-service](https://github.com/khaledsellami/decomp-parsing-service.git).
2. Run the following commands to generate the decomposition of the monolithic application:

   ```sh
   python main.py decompose your_app_name --data /link/to/repository --output /path/to/output
   ```
   You can specify the hyperparameters like the population size and the number of generations through the CLI. For more details, you can use the following command:
   ```sh
    python main.py decompose --help
    ```
   Otherwise, you can start the server and use the gRPC client to generate the decomposition like the Docker image example:
    ```sh
   python main.py start
    ```

### With your own analysis tools or existing data:
1. Generate the analysis data using your own tools.
2. MSExtractor requires 2 input files in parquet. The names of the files depends on the , which have to be placed in the following structure:
   - the call dependency matrix: a NxN matrix representing the calls from each class/method to the others where N is the number of classes/methods. The index must contain the class/method names.
   - The TF-IDF matrix: a NxM matrix representing the TF-IDF vectors of each class/method where N is the number of classes/methods and M is the size of the vocabulary. The index must contain the class/method names.
3. Run the following command to generate the decomposition:
   ```sh
    python main.py decompose your_app_name --calls /path/to/calls.parquet \
        --tfidf /path/to/tfidf.parquet --output /path/to/output --granularity class
    ```





1. Select the source code of the monolithic application to decompose:  
    ```sh
   export MONOPATH="path_to_your_app_src"
   export APPNAME="name_of_the_app"
   ```
2. Use the analysis tool to generate the class and method metadata:
    ```sh
   mkdir -p ./temp_data/$APPNAME/static_analysis
   docker run -it --rm -v $MONOPATH:/input/ -v ./temp_data/$APPNAME/static_analysis:/output/ dcalsel/decomp-java-analysis-service:latest analyze $APPNAME -p /input -o /output
   ```
3. Use the parsing tool to extract the call and semantic data:
    ```sh
   mkdir -p ./temp_data/$APPNAME/parsing_results
   docker run -it --rm -v ./temp_data/$APPNAME/static_analysis/$APPNAME:/input -v ./temp_data/$APPNAME/parsing_results/:/output/ dcalsel/decomp-parsing-service:latest parse $APPNAME -f CSV -d /input/ -o /output/
   ```
4. Prepare the data for MSExtractor:
    ```sh
   python to_msextractor.py --data ./temp_data/$APPNAME/parsing_results/$APPNAME --output ./temp_data/
   ```
5. Create the decomposition with MSExtractor:
    ```sh
   mkdir -p ./temp_data/decompositions
   docker run -it --rm -v ./temp_data/decompositions/:/output -v ./temp_data:/data dcalsel/msextractor:latest decompose $APPNAME --data /data --output /output
   ```
6. The output of MSExtractor can be found in the file "decomposition.json" in the output directly. For example, the auto-generated name of the run is "name_of_the_app_202402021000", the decomposition as well as additional metadata can be found here:
    ```sh
   ls ./temp_data/decompositions/$APPNAME
   cat ./temp_data/decompositions/$APPNAME/name_of_the_app_202402021000/decomposition.json
   ```

Additional details about the CLI can be inspected using the following command:
```sh
   docker run -it --rm dcalsel/msextractor:latest decompose --help
```


### Using a third-party analysis tool
It is possible to use MSExtractor with a different analysis tool with MSExtractor. In this case, MSExtractor can be used in this way:
```sh
   export APPNAME="name_of_the_app"
   mkdir -p ./temp_data/decompositions
   docker run -it --rm -v ./temp_data/decompositions/:/output -v ./path_to_my_data:/data \
          dcalsel/msextractor:latest decompose $APPNAME --calls /data/calls_file.parquet \
          -tfidf /data/tfidf_file.parquet --output /output
```

where "path_to_my_data" is the folder that contains all of the required input. The folder must contain the following elements:
```text
   path_to_my_data/
   ├── calls_file.parquet: a Dataframe with shape NxN representing the calls from each class to the others where N 
   │                   is the number of classes. The index and columns must contain the class names.
   └── tfidf_file.parquet: a Dataframe with shape NxM representing the TF-IDF vectors of each class where N is the 
                           number of classes and M is the size of the vocabulary. The names of the methods as index is required.
```





<!-- LICENSE -->
## License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.




<!-- REFERENCES -->
## References

<a id="1">[1]</a> 
Khaled Sellami, Ali Ouni, Mohamed Aymen Saied, Salah Bouktif, & Mohamed Wiem Mkaouer (2022). 
Improving microservices extraction using evolutionary search. 
Information and Software Technology, 151, 106996.

<!-- CITAION -->
## Citation
If this work was useful for your research, please consider citing it:
```bibtex
@article{SELLAMI2022106996,
   title = {Improving microservices extraction using evolutionary search},
   journal = {Information and Software Technology},
   volume = {151},
   pages = {106996},
   year = {2022},
   issn = {0950-5849},
   doi = {https://doi.org/10.1016/j.infsof.2022.106996},
   url = {https://www.sciencedirect.com/science/article/pii/S0950584922001264},
   author = {Khaled Sellami and Ali Ouni and Mohamed Aymen Saied and Salah Bouktif and Mohamed Wiem Mkaouer},
}
```


<p align="right">(<a href="#readme-top">back to top</a>)</p>